{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn \n",
    "import numpy as np\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "import re\n",
    "import time\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import data using pandas and put into SFrames:\n",
    "papers_data = pd.read_csv('Papers.csv')\n",
    "authors_data = pd.read_csv('Authors.csv')\n",
    "authorId_data = pd.read_csv('PaperAuthors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def given_paperID_give_index(paper_id, paper_data):\n",
    "    return paper_data[paper_data['Id']==paper_id].index[0]\n",
    "#\n",
    "def given_index_give_PaperID(index, paper_data):\n",
    "    return paper_data.iloc[index]['Id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Learning with Symmetric Label Noise: The\\nImportance of Being Unhinged\\n\\nBrendan van Rooyen\\xe2\\x88\\x97,\\xe2\\x80\\xa0\\n\\xe2\\x88\\x97\\n\\nAditya Krishna Menon\\xe2\\x80\\xa0,\\xe2\\x88\\x97\\n\\nThe Australian National University\\n\\n\\xe2\\x80\\xa0\\n\\nRobert C. Williamson\\xe2\\x88\\x97,\\xe2\\x80\\xa0\\n\\nNational ICT Australia\\n\\n{ brendan.vanrooyen, aditya.menon, bob.williamson }@nicta.com.au\\n\\nAbstract\\nConvex potential minimisation is the de facto approach to binary classification.\\nHowever, Long and Servedio [2010] proved that under symmetric label noise\\n(SLN), minimisation of any convex potential over a linear function class can result in classification performance equivalent to random guessing. This ostensibly\\nshows that convex losses are not SLN-robust. In this paper, we propose a convex,\\nclassification-calibrated loss and prove that it is SLN-robust. The loss avoids the\\nLong and Servedio [2010] result by virtue of being negatively unbounded. The\\nloss is a modification of the hinge loss, where one does not clamp at zero; hence,\\nwe call it the unhinged loss. We show that the optimal unhi'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ex_paper_id = 5941\n",
    "Ex_paper_index = given_paperID_give_index(Ex_paper_id, papers_data)\n",
    "papers_data.iloc[Ex_paper_index]['PaperText'][0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    list_of_cleaning_signs = ['\\x0c', '\\n']\n",
    "    for sign in list_of_cleaning_signs:\n",
    "        text = text.replace(sign, ' ')\n",
    "    #text = unicode(text, errors='ignore')\n",
    "    clean_text = re.sub('[^a-zA-Z]+', ' ', text)\n",
    "    return clean_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "papers_data['PaperText_clean'] = papers_data['PaperText'].apply(lambda x: clean_text(x))\n",
    "papers_data['Abstract_clean'] = papers_data['Abstract'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'learning with symmetric label noise the importance of being unhinged brendan van rooyen aditya krishna menon the australian national university robert c williamson national ict australia brendan vanrooyen aditya menon bob williamson nicta com au abstract convex potential minimisation is the de facto approach to binary classification however long and servedio proved that under symmetric label noise sln minimisation of any convex potential over a linear function class can result in classification performance equivalent to random guessing this ostensibly shows that convex losses are not sln robust in this paper we propose a convex classification calibrated loss and prove that it is sln robust the loss avoids the long and servedio result by virtue of being negatively unbounded the loss is a modification of the hinge loss where one does not clamp at zero hence we call it the unhinged loss we show that the optimal unhinged solution is equivalent to that of a strongly regularised svm and is t'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_data.iloc[Ex_paper_index]['PaperText_clean'][0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# here Brandon defines a tokenizer and stemmer which returns the set \n",
    "# of stems in the text that it is passed\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.45 s, sys: 8.06 ms, total: 1.45 s\n",
      "Wall time: 1.51 s\n",
      "CPU times: user 42.9 s, sys: 414 ms, total: 43.3 s\n",
      "Wall time: 43.2 s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Producing tf_idf matrix separately based on Abstract\n",
    "tfidf_vectorizer_Abstract = TfidfVectorizer(max_df=0.95, max_features=200000,\n",
    "                                 min_df=0.05, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
    "%time tfidf_matrix_Abstract = tfidf_vectorizer_Abstract.fit_transform(papers_data['Abstract_clean'])\n",
    "\n",
    "# Producing tf_idf matrix separately based on PaperText\n",
    "tfidf_vectorizer_PaperText = TfidfVectorizer(max_df=0.9, max_features=200000,\n",
    "                                 min_df=0.1, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
    "%time tfidf_matrix_PaperText = tfidf_vectorizer_PaperText.fit_transform(papers_data['PaperText_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "terms_Abstract = tfidf_vectorizer_Abstract.get_feature_names()\n",
    "terms_PaperText = tfidf_vectorizer_Abstract.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_tfidf_feats(row, terms, top_n=25):\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(terms[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df['feature']\n",
    "def given_paperID_give_keywords(paper_data, tfidfMatrix, terms, paper_id, top_n=20):\n",
    "    row_id = given_paperID_give_index(paper_id, paper_data)\n",
    "    row = np.squeeze(tfidfMatrix[row_id].toarray())\n",
    "    return top_tfidf_feats(row, terms, top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords based on Abstract:\n",
      "0            loss\n",
      "1          convex\n",
      "2          robust\n",
      "3         classif\n",
      "4          strong\n",
      "5           solut\n",
      "6           prove\n",
      "7             ani\n",
      "8    paper propos\n",
      "9          result\n",
      "Name: feature, dtype: object\n"
     ]
    }
   ],
   "source": [
    "paper_id_example = 5941\n",
    "print (\"Keywords based on Abstract:\")\n",
    "print (given_paperID_give_keywords(papers_data, tfidf_matrix_Abstract,\n",
    "                                  terms_Abstract, paper_id_example, top_n = 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "# Based on Abstract\n",
    "num_neighbors = 4\n",
    "nbrs_Abstract = NearestNeighbors(n_neighbors=num_neighbors,\n",
    "                                 algorithm='auto').fit(tfidf_matrix_Abstract)\n",
    "distances_Abstract, indices_Abstract = nbrs_Abstract.kneighbors(tfidf_matrix_Abstract)\n",
    "# Based on PaperText\n",
    "nbrs_PaperText = NearestNeighbors(n_neighbors=num_neighbors,\n",
    "                                  algorithm='auto').fit(tfidf_matrix_PaperText)\n",
    "distances_PaperText, indices_PaperText = nbrs_PaperText.kneighbors(tfidf_matrix_PaperText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nbrs of the example paper based on Abstract similarity: array([  1,  87, 301, 112])\n",
      "Nbrs of the example paper based on PaperText similarity: array([  1, 125, 112, 148])\n"
     ]
    }
   ],
   "source": [
    "print (\"Nbrs of the example paper based on Abstract similarity: %r\" % indices_Abstract[1])\n",
    "print (\"Nbrs of the example paper based on PaperText similarity: %r\" % indices_PaperText[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Abstract of the example paper is:\n",
      "\n",
      "Convex potential minimisation is the de facto approach to binary classification. However, Long and Servedio [2008] proved that under symmetric label noise (SLN), minimisation of any convex potential over a linear function class can result in classification performance equivalent to random guessing. This ostensibly shows that convex losses are not SLN-robust. In this paper, we propose a convex, classification-calibrated loss and prove that it is SLN-robust. The loss avoids the Long and Servedio [2008] result by virtue of being negatively unbounded. The loss is a modification of the hinge loss, where one does not clamp at zero; hence, we call it the unhinged loss. We show that the optimal unhinged solution is equivalent to that of a strongly regularised SVM, and is the limiting solution for any convex potential; this implies that strong l2 regularisation makes most standard learners SLN-robust. Experiments confirm the unhinged lossâ€™ SLN-robustness.\n",
      "The Abstract of the similar papers are:\n",
      "\n",
      "Neighbor No. 1 has following abstract: \n",
      "\n",
      "The framework of online learning with memory naturally captures learning problems with temporal effects, and was previously studied for the experts setting. In this work we extend the notion of learning with memory to the general Online Convex Optimization (OCO) framework, and present two algorithms that attain low regret. The first algorithm applies to Lipschitz continuous loss functions, obtaining optimal regret bounds for both convex and strongly convex losses. The second algorithm attains the optimal regret bounds and applies more broadly to convex losses without requiring Lipschitz continuity, yet is more complicated to implement. We complement the theoretic results with two applications: statistical arbitrage in finance, and multi-step ahead prediction in statistics.\n",
      "\n",
      "\n",
      "Neighbor No. 2 has following abstract: \n",
      "\n",
      "Multivariate loss functions are used to assess performance in many modern prediction tasks, including information retrieval and ranking applications. Convex approximations are typically optimized in their place to avoid NP-hard empirical risk minimization problems. We propose to approximate the training data instead of the loss function by posing multivariate prediction as an adversarial game between a loss-minimizing prediction player and a loss-maximizing evaluation player constrained to match specified properties of training data. This avoids the non-convexity of empirical risk minimization, but game sizes are exponential in the number of predicted variables. We overcome this intractability using the double oracle constraint generation method. We demonstrate the efficiency and predictive performance of our approach on tasks evaluated using the precision at k, the F-score and the discounted cumulative gain.\n",
      "\n",
      "\n",
      "Neighbor No. 3 has following abstract: \n",
      "\n",
      "Modern prediction problems arising in multilabel learning and learning to rank pose unique challenges to the classical theory of supervised learning. These problems have large prediction and label spaces of a combinatorial nature and involve sophisticated loss functions. We offer a general framework to derive mistake driven online algorithms and associated loss bounds.  The key ingredients in our framework are a general loss function, a general vector space representation of predictions, and a notion of margin with respect to a general norm. Our general algorithm, Predtron, yields the perceptron algorithm and its variants when instantiated on classic problems such as binary classification, multiclass classification, ordinal regression, and multilabel classification.  For multilabel ranking and subset ranking, we derive novel algorithms, notions of margins, and loss bounds. A simulation study confirms the behavior predicted by our bounds and demonstrates the flexibility of the design choices in our framework.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Ex_paper_id = 5941\n",
    "Ex_index = given_paperID_give_index(Ex_paper_id, papers_data)\n",
    "print (\"The Abstract of the example paper is:\\n\")\n",
    "print (papers_data.iloc[indices_Abstract[Ex_index][0]]['Abstract'])\n",
    "print (\"The Abstract of the similar papers are:\\n\")\n",
    "for i in range(1, len(indices_Abstract[Ex_index])):\n",
    "    print (\"Neighbor No. %r has following abstract: \\n\" % i)\n",
    "    print (papers_data.iloc[indices_Abstract[Ex_index][i]]['Abstract'])\n",
    "    print (\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
