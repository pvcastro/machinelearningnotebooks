{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the pandas package, then use the \"read_csv\" function to read\n",
    "# the labeled training data\n",
    "import pandas as pd  \n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 3)\n",
      "['id' 'sentiment' 'review']\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"./Kaggle/Bag of Words/labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "print train.shape\n",
    "print train.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# defines a tokenizer and stemmer which returns the set of stems in the text that it is passed\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    stems = [stemmer.stem(t) for t in tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_review( raw_review ):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(raw_review).get_text() \n",
    "    #\n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    #\n",
    "    # 3. Convert to lower case\n",
    "    return letters_only.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tfidf_features(number_features = 5000):\n",
    "    print \"Creating the bag of words...\\n\"\n",
    "    # Initialize the \"CountVectorizer\" object, which is scikit-learn's bag of words tool.  \n",
    "    vectorizer = TfidfVectorizer(tokenizer = tokenize_and_stem,    \\\n",
    "                                 preprocessor = preprocess_review, \\\n",
    "                                 stop_words = stopwords,   \\\n",
    "                                 max_features = number_features, \\\n",
    "                                 ngram_range=(1,3), \\\n",
    "                                 use_idf=True) \n",
    "\n",
    "    # fit_transform() does two functions: First, it fits the model\n",
    "    # and learns the vocabulary; second, it transforms our training data\n",
    "    # into feature vectors. The input to fit_transform should be a list of \n",
    "    # strings.\n",
    "    train_data_features = vectorizer.fit_transform(train['review'])\n",
    "    # Numpy arrays are easy to work with, so convert the result to an \n",
    "    # array\n",
    "    return train_data_features.toarray(), vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_random_forest(number_features = 5000):\n",
    "    # Get the features\n",
    "    train_data_features, vectorizer = get_tfidf_features(number_features=number_features)\n",
    "    #\n",
    "    print \"Training the random forest...\"\n",
    "    # Initialize a Random Forest classifier with 100 trees\n",
    "    forest = RandomForestClassifier(n_estimators = 100) \n",
    "    # Fit the forest to the training set, using the bag of words as \n",
    "    # features and the sentiment labels as the response variable\n",
    "    #\n",
    "    # This may take a few minutes to run\n",
    "    return forest.fit( train_data_features, train[\"sentiment\"] ), vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict_test_data(number_features = 5000):\n",
    "    # Train the Random Forest model\n",
    "    forest, vectorizer = train_random_forest(number_features=number_features)\n",
    "    # Read the test data\n",
    "    test = pd.read_csv(\"Kaggle/Bag of Words/testData.tsv\", header=0, delimiter=\"\\t\", \\\n",
    "                       quoting=3 )\n",
    "    # Get tf-idf features for the test set, and convert to a numpy array\n",
    "    test_data_features = vectorizer.transform(test[\"review\"])\n",
    "    test_data_features = test_data_features.toarray()\n",
    "\n",
    "    # Use the random forest to make sentiment label predictions\n",
    "    result = forest.predict(test_data_features)\n",
    "\n",
    "    # Copy the results to a pandas dataframe with an \"id\" column and\n",
    "    # a \"sentiment\" column\n",
    "    output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
    "\n",
    "    # Use pandas to write the comma-separated output file\n",
    "    output.to_csv( \"Kaggle/Bag of Words/Bag_of_Words_model_submission5_\" + str(number_features) + \".csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words...\n",
      "\n",
      "Training the random forest...\n",
      "Creating the bag of words...\n",
      "\n",
      "Training the random forest...\n",
      "Creating the bag of words...\n",
      "\n",
      "Training the random forest...\n",
      "Creating the bag of words...\n",
      "\n",
      "Training the random forest...\n",
      "Creating the bag of words...\n",
      "\n",
      "Training the random forest...\n",
      "Creating the bag of words...\n",
      "\n",
      "Training the random forest...\n",
      "Creating the bag of words...\n",
      "\n",
      "Training the random forest...\n",
      "Creating the bag of words...\n",
      "\n",
      "Training the random forest...\n",
      "Creating the bag of words...\n",
      "\n",
      "Training the random forest...\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 11):\n",
    "    predict_test_data(number_features=(i * 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words...\n",
      "\n",
      "Training the random forest...\n"
     ]
    }
   ],
   "source": [
    "predict_test_data(number_features=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
